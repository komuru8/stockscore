import logging
from typing import Dict, Any, Optional

class RelativeScoringEngine:
    """
    Relative scoring engine that compares stock metrics against baseline values
    Implements user-specified 5-tier evaluation system with proper score distribution
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Baseline values for comparison (market average values)
        self.baselines = {
            'pe_ratio': 20.0,        # Market average PER
            'pb_ratio': 1.5,         # Market average PBR
            'roe': 12.0,             # Market average ROE (%)
            'roa': 6.0,              # Market average ROA (%)
            'dividend_yield': 2.5,   # Market average dividend yield (%)
            'profit_margins': 15.0,  # Market average profit margin (%)
            'debt_to_equity': 50.0,  # Market average D/E ratio
            'current_ratio': 1.8,    # Market average current ratio
            'earnings_growth': 8.0,  # Market average earnings growth (%)
            'revenue_growth': 6.0    # Market average revenue growth (%)
        }
        
        # Mode configurations
        self.mode_configs = {
            'beginner': {
                'metrics': ['pe_ratio', 'dividend_yield'],
                'max_points_per_metric': 50,
                'total_points': 100
            },
            'intermediate': {
                'metrics': ['pe_ratio', 'pb_ratio', 'roe', 'roa', 'dividend_yield', 
                           'profit_margins', 'debt_to_equity', 'current_ratio', 
                           'earnings_growth', 'revenue_growth'],
                'max_points_per_metric': 10,
                'total_points': 100
            }
        }
    
    def calculate_score(self, stock_data: Dict, mode: str = 'intermediate') -> Dict:
        """Calculate relative score based on mode"""
        try:
            if mode not in self.mode_configs:
                mode = 'intermediate'  # Default fallback
                
            config = self.mode_configs[mode]
            metrics = config['metrics']
            max_points = config['max_points_per_metric']
            
            individual_scores = {}
            total_score = 0
            
            for metric in metrics:
                score = self._calculate_metric_score(stock_data, metric, max_points)
                individual_scores[metric] = score
                total_score += score
            
            # Generate assessment and recommendation
            assessment = self._generate_assessment(total_score)
            recommendation = self._get_investment_recommendation(total_score)
            rank = self._get_rank(total_score)
            color = self._get_color_scale(total_score)
            
            return {
                'total_score': round(total_score, 1),
                'individual_scores': individual_scores,
                'assessment': assessment,
                'recommendation': recommendation,
                'rank': rank,
                'color': color,
                'mode': mode,
                'max_possible_score': config['total_points']
            }
            
        except Exception as e:
            self.logger.error(f"Error calculating relative score: {e}")
            return self._get_error_result()
    
    def _calculate_metric_score(self, stock_data: Dict, metric: str, max_points: int) -> float:
        """Calculate score for individual metric using relative comparison"""
        try:
            value = stock_data.get(metric)
            
            # Handle missing data - return "normal" score (5 points for 10-point max, 25 for 50-point max)
            if value is None or not isinstance(value, (int, float)):
                return max_points * 0.5  # Normal score (50% of max)
            
            baseline = self.baselines.get(metric)
            if baseline is None:
                return max_points * 0.5  # Fallback to normal
            
            # Convert percentage values if needed
            if metric in ['roe', 'roa', 'dividend_yield', 'profit_margins', 'earnings_growth', 'revenue_growth']:
                if value < 1:  # Convert decimal to percentage
                    value = value * 100
            
            # Calculate relative performance
            if metric in ['pe_ratio', 'pb_ratio', 'debt_to_equity']:
                # Lower is better metrics
                relative_performance = (baseline - value) / baseline
            else:
                # Higher is better metrics
                relative_performance = (value - baseline) / baseline
            
            # Apply 5-tier scoring system based on relative performance
            if relative_performance >= 0.20:      # +20% or better
                return max_points * 1.0          # ÈùûÂ∏∏„Å´ËâØ„ÅÑ: 10ÁÇπ (100%) or 50ÁÇπ
            elif relative_performance >= 0.10:    # +10% to +20%
                return max_points * 0.8          # ËâØ„ÅÑ: 8ÁÇπ (80%) or 40ÁÇπ
            elif relative_performance >= -0.10:   # ¬±10%
                return max_points * 0.5          # ÊôÆÈÄö: 5ÁÇπ (50%) or 25ÁÇπ
            elif relative_performance >= -0.20:   # -10% to -20%
                return max_points * 0.2          # ÊÇ™„ÅÑ: 2ÁÇπ (20%) or 10ÁÇπ
            else:                                 # -20% or worse
                return max_points * 0.0          # ÈùûÂ∏∏„Å´ÊÇ™„ÅÑ: 0ÁÇπ or 0ÁÇπ
                
        except Exception as e:
            self.logger.error(f"Error calculating score for {metric}: {e}")
            return max_points * 0.5  # Return normal score on error
    
    def _generate_assessment(self, score: float) -> str:
        """Generate human-readable assessment"""
        if score >= 80:
            return "üöÄ Âº∑„ÅÑË≤∑„ÅÑÊé®Â•® / Strong Buy - ÂÑ™ÁßÄ„Å™Ë≤°ÂãôÊåáÊ®ô"
        elif score >= 70:
            return "‚úÖ Ë≤∑„ÅÑÊé®Â•® / Buy - ËâØÂ•Ω„Å™ÊäïË≥áÊ©ü‰ºö"
        elif score >= 60:
            return "‚ûñ ‰∏≠Á´ã„Éª‰øùÊúâ / Hold - Âπ≥ÂùáÁöÑ„Å™ performance"
        elif score >= 40:
            return "‚ö†Ô∏è ÊÖéÈáç / Caution - ÊÖéÈáç„Å™Ê§úË®é„ÅåÂøÖË¶Å"
        else:
            return "‚ùå ÈùûÊé®Â•® / Not Recommended - ÊäïË≥á„É™„Çπ„ÇØ„ÅåÈ´ò„ÅÑ"
    
    def _get_investment_recommendation(self, score: float) -> str:
        """Get investment recommendation based on score"""
        if score >= 80:
            return "üöÄ Âº∑„ÅÑË≤∑„ÅÑÊé®Â•®"
        elif score >= 70:
            return "‚úÖ Ë≤∑„ÅÑÊé®Â•®"  
        elif score >= 60:
            return "‚ûñ ‰∏≠Á´ã„Éª‰øùÊúâ"
        elif score >= 40:
            return "‚ö†Ô∏è ÊÖéÈáç"
        else:
            return "‚ùå ÈùûÊé®Â•®"
    
    def _get_rank(self, score: float) -> str:
        """Get rank based on score"""
        if score >= 90:
            return "S"
        elif score >= 80:
            return "A"
        elif score >= 60:
            return "B"
        elif score >= 40:
            return "C"
        else:
            return "D"
    
    def _get_color_scale(self, score: float) -> str:
        """Get color code for visualization"""
        if score >= 80:
            return "#4CAF50"  # Green - Excellent
        elif score >= 70:
            return "#8BC34A"  # Light Green - Good
        elif score >= 60:
            return "#FFC107"  # Yellow - Hold
        elif score >= 40:
            return "#FF9800"  # Orange - Caution
        else:
            return "#F44336"  # Red - Poor
    
    def _get_error_result(self) -> Dict:
        """Return error result"""
        return {
            'total_score': 0,
            'individual_scores': {},
            'assessment': "‚ùå „Éá„Éº„ÇøÂèñÂæó„Ç®„É©„Éº / Data Error",
            'recommendation': "‚ùå Ë©ï‰æ°‰∏çÂèØ",
            'rank': "E",
            'color': "#9E9E9E",
            'mode': 'error',
            'max_possible_score': 100
        }
    
    def update_baselines(self, **kwargs):
        """Update baseline values for comparison"""
        for key, value in kwargs.items():
            if key in self.baselines:
                self.baselines[key] = value
                self.logger.info(f"Updated baseline for {key} to {value}")